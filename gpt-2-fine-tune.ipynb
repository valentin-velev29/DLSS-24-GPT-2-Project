{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:13.515846Z","iopub.status.busy":"2024-06-25T18:31:13.515467Z","iopub.status.idle":"2024-06-25T18:31:25.516096Z","shell.execute_reply":"2024-06-25T18:31:25.514986Z","shell.execute_reply.started":"2024-06-25T18:31:13.515800Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n","\n","\n","from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel, set_seed\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm\n","\n","from datasets import load_from_disk\n","\n","# Golbal variables\n","SEED_VAL = 42\n","set_seed(SEED_VAL)\n","\n","MODEL_NAME = \"gpt2-medium\"\n","BOS_TOKEN = '<|startoftext|>'\n","EOS_TOKEN = '<|endoftext|>'\n","PAD_TOKEN = '<|pad|>'\n","MAX_LENGTH = 1024\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['text'],\n","    num_rows: 100000\n","})"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["dataset = load_from_disk(\"/home/pop544167/DSP_TK_SoSe24/sub_project/Data/trainset.hf\")\n","dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:51:48.977345Z","iopub.status.busy":"2024-06-25T18:51:48.976574Z","iopub.status.idle":"2024-06-25T18:51:48.995598Z","shell.execute_reply":"2024-06-25T18:51:48.994335Z","shell.execute_reply.started":"2024-06-25T18:51:48.977296Z"},"trusted":true},"outputs":[],"source":["texts=dataset['text'] \n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.033855Z","iopub.status.busy":"2024-06-25T18:31:32.033591Z","iopub.status.idle":"2024-06-25T18:31:32.609476Z","shell.execute_reply":"2024-06-25T18:31:32.608355Z","shell.execute_reply.started":"2024-06-25T18:31:32.033833Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1108 > 1024). Running this sequence through the model will result in indexing errors\n","/tmp/ipykernel_2435663/2849366678.py:14: UserWarning: \n","\n","`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n","\n","Please adapt your code to use either `displot` (a figure-level function with\n","similar flexibility) or `histplot` (an axes-level function for histograms).\n","\n","For a guide to updating your code to use the new functions, please see\n","https://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n","\n","  sns.distplot(doc_lengths)\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN)\n","\n","doc_lengths = []\n","\n","for text in texts:\n","\n","    # get token count distribution including special tokens to asses context size issues\n","    tokens = tokenizer.encode(text, add_special_tokens=True, truncation=False, padding=False)\n","\n","    doc_lengths.append(len(tokens))\n","\n","doc_lengths = np.array(doc_lengths)\n","\n","sns.distplot(doc_lengths)\n","plt.close()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.611418Z","iopub.status.busy":"2024-06-25T18:31:32.610994Z","iopub.status.idle":"2024-06-25T18:31:32.617573Z","shell.execute_reply":"2024-06-25T18:31:32.616706Z","shell.execute_reply.started":"2024-06-25T18:31:32.611382Z"},"trusted":true},"outputs":[{"data":{"text/plain":["3e-05"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# the max token length   \n","len(doc_lengths[doc_lengths > MAX_LENGTH])/len(doc_lengths)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.619150Z","iopub.status.busy":"2024-06-25T18:31:32.618840Z","iopub.status.idle":"2024-06-25T18:31:32.630522Z","shell.execute_reply":"2024-06-25T18:31:32.629396Z","shell.execute_reply.started":"2024-06-25T18:31:32.619125Z"},"trusted":true},"outputs":[{"data":{"text/plain":["34.40322"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["np.average(doc_lengths)"]},{"cell_type":"markdown","metadata":{},"source":["Since only a few documents are outside of the optimal context size of 1024. Therefore we can just use truncation instead of any chunking that might mislead the model."]},{"cell_type":"markdown","metadata":{},"source":["# GPT2 Tokenizer"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Base BOS: <|endoftext|>\n","Base EOS: <|endoftext|>\n","Base PAD: None\n"]}],"source":["tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n","print(f'Base BOS: {tokenizer.bos_token}')\n","print(f'Base EOS: {tokenizer.eos_token}')\n","print(f'Base PAD: {tokenizer.pad_token}')\n","\n","# adjust global Variables according to models naming of special tokens"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:32.632490Z","iopub.status.busy":"2024-06-25T18:31:32.631822Z","iopub.status.idle":"2024-06-25T18:31:33.814137Z","shell.execute_reply":"2024-06-25T18:31:33.813129Z","shell.execute_reply.started":"2024-06-25T18:31:32.632462Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The max model length is 1024 for this model\n","The beginning of sequence token <|startoftext|> token has the id 50257\n","The end of sequence token <|endoftext|> has the id 50256\n","The padding token <|pad|> has the id 50258\n"]}],"source":["# Load the GPT tokenizer.\n","tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN) #gpt2-medium\n","\n","print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n","print(\"The beginning of sequence token {} token has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n","print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n","print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:31:33.818164Z","iopub.status.busy":"2024-06-25T18:31:33.817832Z","iopub.status.idle":"2024-06-25T18:31:33.826954Z","shell.execute_reply":"2024-06-25T18:31:33.826021Z","shell.execute_reply.started":"2024-06-25T18:31:33.818136Z"},"trusted":true},"outputs":[],"source":["class TextDataset(Dataset):\n","\n","  def __init__(self, txt_list, tokenizer, max_length):\n","    # Store features\n","    self.tokenizer = tokenizer\n","    self.input_ids = []\n","    self.attn_masks = []\n","\n","    for txt in txt_list:\n","      # Encode text in between BOS and EOS\n","      encodings_dict = tokenizer(BOS_TOKEN + txt + EOS_TOKEN, truncation=True, max_length=max_length, padding=\"max_length\")\n","\n","      # Make features tensors and store\n","      self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n","      self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n","    \n","  def __len__(self):\n","    return len(self.input_ids)\n","\n","  def __getitem__(self, idx):\n","    return self.input_ids[idx], self.attn_masks[idx] \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# # Use GPU 0\n","# import os\n","# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n","# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","#torch.cuda.set_device(0)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/pop544167/.pyenv/versions/3.11.4/envs/project2/lib/python3.11/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch 1: 100%|██████████| 50/50 [00:29<00:00,  1.68it/s]\n","Epoch 2: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n","Epoch 3: 100%|██████████| 50/50 [00:30<00:00,  1.66it/s]\n","Epoch 4: 100%|██████████| 50/50 [00:30<00:00,  1.65it/s]\n","Epoch 5: 100%|██████████| 50/50 [00:30<00:00,  1.67it/s]\n"]}],"source":["def fine_tune_gpt2(train_data, output_dir=None):\n","    \n","    training_stats = []\n","    args = {}  # Initialize the args dictionary\n","    \n","    # Set the seed value all over the place to make this reproducible.\n","    set_seed(SEED_VAL)\n","    \n","    # Update args with seed and model parameters\n","    args['seed_val'] = SEED_VAL\n","    args['model_name'] = MODEL_NAME\n","    \n","    # Load GPT-2 model and tokenizer\n","    tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME, bos_token=BOS_TOKEN, eos_token=EOS_TOKEN, pad_token=PAD_TOKEN) # add special tokens \n","    model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n","    model.resize_token_embeddings(len(tokenizer)) # resize embeddings according to the updated tokenizer\n","\n","    # Update args with tokenizer parameters\n","    args['bos_token'] = '50257'\n","    args['eos_token'] = '50256'\n","    args['pad_token'] = '50258'\n","    args['max_length'] = MAX_LENGTH #1024\n","    args['batch_size'] = 2\n","    args['learning_rate'] = 5e-4\n","    args['eps'] = 1e-8\n","    args['num_train_epochs'] = 5\n","    args['num_warmup_steps'] = 1e2\n","    \n","    # Load training dataset\n","    train_dataset = TextDataset(\n","        tokenizer=tokenizer,\n","        txt_list=train_data,\n","        max_length=args['max_length']\n","    )\n","\n","    # Create data loader\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=RandomSampler(train_dataset),\n","        batch_size=args['batch_size']\n","    )\n","\n","    # Set up optimizer and learning rate scheduler\n","    optimizer = AdamW(model.parameters(), lr=args['learning_rate'], eps=args['eps'])\n","    \n","    total_steps = len(train_dataloader) * args['num_train_epochs'] # calculate number of total steps\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=args['num_warmup_steps'],\n","        num_training_steps=total_steps\n","    )\n","\n","    # Move model to GPU if available\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","    \n","    args['device'] = device\n","    \n","    # Training loop\n","    model.train()\n","    # Iterate over each epoch\n","    for epoch in range(args['num_train_epochs']):\n","        # Expected completion time for one epoch \n","        epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n","        # Set loss per batch to 0\n","        batch_loss = 0\n","        # Iterate over each batch\n","        for step, batch in enumerate(epoch_iterator):\n","            torch.cuda.empty_cache() # clean GPU cache\n","            inputs, attention_masks = batch # extract features\n","            # Move features to GPU\n","            inputs = inputs.to(device)\n","            attention_masks = attention_masks.to(device)\n","\n","            # Calculate probabilities\n","            outputs = model(inputs, attention_mask=attention_masks, labels=inputs)\n","            # Save loss\n","            loss = outputs.loss\n","            batch_loss += loss.item()\n","            # Perform backpropagation\n","            loss.backward()\n","\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","        # Calculate average loss for one epoch\n","        avg_train_loss = batch_loss / len(train_dataloader)\n","        training_stats.append(\n","            {\n","                'epoch': epoch + 1,\n","                'Training Loss': avg_train_loss,\n","            }\n","        )\n","    \n","    return args, training_stats, model\n","args, training_stats, model = fine_tune_gpt2(train_data=texts[:100])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-06-25T18:57:24.177939Z","iopub.status.busy":"2024-06-25T18:57:24.177060Z"},"trusted":true},"outputs":[],"source":["# def fine_tune_gpt2(model_name, train_data):\n","#     training_stats = []\n","#     # Set the seed value all over the place to make this reproducible.\n","#     seed_val = 42\n","\n","#     random.seed(seed_val)\n","#     np.random.seed(seed_val)\n","#     torch.manual_seed(seed_val)\n","#     torch.cuda.manual_seed_all(seed_val)\n","   \n","#     # Load GPT-2 model and tokenizer\n","#     tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') \n","#     model = GPT2LMHeadModel.from_pretrained(model_name)\n","#     model.resize_token_embeddings(len(tokenizer))\n","\n","#     # Load training dataset\n","#     train_dataset = TextDataset(\n","#         tokenizer=tokenizer,\n","#         txt_list = train_data,\n","#         max_length=768\n","#     )\n","\n","#     # Create data loader\n","#     train_dataloader = DataLoader(\n","#         train_dataset,\n","#         sampler=RandomSampler(train_dataset),\n","#         batch_size=2\n","#     )\n","\n","#     # Set up optimizer and learning rate scheduler\n","#     optimizer = AdamW(model.parameters(), lr=5e-4, eps=1e-8)\n","    \n","    \n","    \n","#     total_steps = len(train_dataloader) * 5  # 5 epochs\n","#     scheduler = get_linear_schedule_with_warmup(\n","#         optimizer,\n","#         num_warmup_steps=1e2,\n","#         num_training_steps=total_steps\n","#     )\n","\n","#     # Move model to GPU if available\n","#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#     model.to(device)\n","\n","#     # Training loop\n","    \n","#     model.train()\n","#     for epoch in range(5):  # num_train_epochs\n","#         epoch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n","#         batch_loss = 0\n","#         for step, batch in enumerate(epoch_iterator):\n","                       \n","#             inputs, attention_masks = batch\n","#             inputs = inputs.to(device)\n","#             attention_masks = attention_masks.to(device)\n","\n","#             outputs = model(inputs, attention_mask=attention_masks, labels=inputs)\n","            \n","#             loss = outputs.loss\n","#             batch_loss += loss.item()\n","#             loss.backward()\n","\n","#             optimizer.step()\n","#             scheduler.step()\n","#             optimizer.zero_grad()\n","            \n","#         avg_train_loss = loss/len(train_dataloader)\n","#         training_stats.append(\n","#         {\n","#             'epoch': epoch+1,\n","#             'Training Loss': avg_train_loss,\n","#             #'Valid. Loss': avg_val_loss,\n","#             #'Training Time': training_time,\n","#             #'Validation Time': validation_time\n","#         }\n","#     )\n","\n","#     return training_stats, model\n","\n","# # Uncomment the following line to run the function\n","# training_stats, model  = fine_tune_gpt2('gpt2', train_data=texts[:100])"]},{"cell_type":"markdown","metadata":{},"source":["# Display Model Info"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:37:30.523338Z","iopub.status.idle":"2024-06-25T18:37:30.523637Z","shell.execute_reply":"2024-06-25T18:37:30.523499Z","shell.execute_reply.started":"2024-06-25T18:37:30.523487Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The GPT-2 model has 292 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","transformer.wte.weight                                  (50259, 1024)\n","transformer.wpe.weight                                  (1024, 1024)\n","\n","==== First Transformer ====\n","\n","transformer.h.0.ln_1.weight                                  (1024,)\n","transformer.h.0.ln_1.bias                                    (1024,)\n","transformer.h.0.attn.c_attn.weight                      (1024, 3072)\n","transformer.h.0.attn.c_attn.bias                             (3072,)\n","transformer.h.0.attn.c_proj.weight                      (1024, 1024)\n","transformer.h.0.attn.c_proj.bias                             (1024,)\n","transformer.h.0.ln_2.weight                                  (1024,)\n","transformer.h.0.ln_2.bias                                    (1024,)\n","transformer.h.0.mlp.c_fc.weight                         (1024, 4096)\n","transformer.h.0.mlp.c_fc.bias                                (4096,)\n","transformer.h.0.mlp.c_proj.weight                       (4096, 1024)\n","transformer.h.0.mlp.c_proj.bias                              (1024,)\n","\n","==== Output Layer ====\n","\n","transformer.ln_f.weight                                      (1024,)\n","transformer.ln_f.bias                                        (1024,)\n"]}],"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(model.named_parameters())\n","\n","print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:2]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[2:14]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-2:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"]},{"cell_type":"markdown","metadata":{},"source":["# Saving & Loading Fine-Tuned Model"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.status.busy":"2024-06-25T18:37:30.524603Z","iopub.status.idle":"2024-06-25T18:37:30.524939Z","shell.execute_reply":"2024-06-25T18:37:30.524772Z","shell.execute_reply.started":"2024-06-25T18:37:30.524759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Saving model to ./models/GPT\n"]}],"source":["# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './models/GPT2-medium'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","torch.save(args, os.path.join(output_dir, 'training_args.bin'))"]},{"cell_type":"markdown","metadata":{},"source":["### Save training loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training_stats = [stat.cpu() for stat in training_stats]\n","pd.set_option('display.precision', 2)\n","df = pd.DataFrame(training_stats)\n","#df['Training Loss'] = [loss.cpu().item() for loss in df['Training Loss']] # move to cpu\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(df['epoch'], df['Training Loss'])\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.show()\n","plt.savefig(output_dir +'/training_loss.png')\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5271245,"sourceId":8771507,"sourceType":"datasetVersion"},{"datasetId":5280684,"sourceId":8784286,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}
