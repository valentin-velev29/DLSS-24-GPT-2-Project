{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9196177,"sourceType":"datasetVersion","datasetId":5559574}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-19T21:59:37.534676Z","iopub.execute_input":"2024-08-19T21:59:37.535110Z","iopub.status.idle":"2024-08-19T21:59:38.646802Z","shell.execute_reply.started":"2024-08-19T21:59:37.535076Z","shell.execute_reply":"2024-08-19T21:59:38.645736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ['HF_TOKEN']=\"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\"\nos.environ['HUGGINGFACEHUB_API_TOKEN']= \"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\" \nimport transformers\nimport torch\n\ntorch.cuda.empty_cache()\ntorch.manual_seed(42)\n\nmodel_id = \"meta-llama/Meta-Llama-3-8B\"\n\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    tokenizer=tokenizer,\n    model_kwargs={\"torch_dtype\":torch.float16},\n    device_map=\"cuda\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:59:38.648522Z","iopub.execute_input":"2024-08-19T21:59:38.648954Z","iopub.status.idle":"2024-08-19T22:02:41.732004Z","shell.execute_reply.started":"2024-08-19T21:59:38.648926Z","shell.execute_reply":"2024-08-19T22:02:41.730986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 50\nbatches = 3350 // batch_size\nprint(batches)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T23:07:41.849742Z","iopub.execute_input":"2024-08-19T23:07:41.850161Z","iopub.status.idle":"2024-08-19T23:07:41.855838Z","shell.execute_reply.started":"2024-08-19T23:07:41.850129Z","shell.execute_reply":"2024-08-19T23:07:41.854792Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"67\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_perplexity(text, model, tokenizer):\n    # Tokenize the input text\n    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate labels (using input_ids as the labels for language modeling)\n    input_ids = inputs['input_ids']\n    \n    # Compute the loss\n    with torch.no_grad():  # No need to track gradients\n        outputs = model(input_ids, labels=input_ids)\n    \n    #print(outputs)\n    # Extract the loss\n    loss = outputs.loss\n    print(loss)\n\n    # Convert the loss to perplexity\n    perplexity = torch.exp(loss).item()\n    return perplexity","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:03:03.946774Z","iopub.execute_input":"2024-08-19T22:03:03.948027Z","iopub.status.idle":"2024-08-19T22:03:03.954928Z","shell.execute_reply.started":"2024-08-19T22:03:03.947979Z","shell.execute_reply":"2024-08-19T22:03:03.953796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best-performing instruction in prompt:\n\nprompts = []\nprompts.append(\"The following is a customer review for a T-shirt: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* This is another customer review for a T-shirt: *\")\nprompts.append(\"Customer Review for a T-shirt: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* Another customer review for a T-shirt: *\")\nprompts.append(\"Review for a T-shirt: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* Another review for a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* Another review of a T-shirt: *\")\nprompts.append(\"T-shirt Review: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* Another T-Shirt review: *\")\n\nfor i in range(0, len(prompts)):\n    print(f\"Prompt {i+1}: perplexity: {calculate_perplexity(prompts[i], pipeline.model, pipeline.tokenizer)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:03:09.605740Z","iopub.execute_input":"2024-08-19T22:03:09.606218Z","iopub.status.idle":"2024-08-19T22:03:11.433680Z","shell.execute_reply.started":"2024-08-19T22:03:09.606186Z","shell.execute_reply":"2024-08-19T22:03:11.432657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Positive examples in prompt","metadata":{}},{"cell_type":"code","source":"prompts = []\n\nprompts.append(\"Here is a review of a T-shirt: *I absolutely love this T-shirt! The material is so soft, and it feels amazing against the skin. I've washed it multiple times, and it still looks brand new. The fit is perfect, and I get compliments every time I wear it. This has become my favorite go-to shirt for both casual and semi-casual outings. Highly recommended!* Another review of a T-shirt:*\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt exceeded my expectations! The quality of the fabric is outstanding, and it's so comfortable to wear. I love how it retains its shape even after several washes. The color hasn't faded at all, and it fits perfectly. I will definitely be buying more in different colors!* Another review of a T-shirt:*\")\nprompts.append(\"Here is a review of a T-shirt: *I am so happy with this purchase! The shirt is soft, breathable, and feels luxurious. The fit is spot-on, and it drapes nicely on my body. I can dress it up with a blazer or down with jeans, making it super versatile. I've received so many compliments on it!* Another review of a T-shirt:*\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt is fantastic! The fabric is high-quality, and it's so comfortable to wear all day. I love that it's both lightweight and durable, and the color stays vibrant even after several washes. I'm very impressed and will definitely be buying more!* Another review of a T-shirt:*\")\nprompts.append(\"Here is a review of a T-shirt: *I can't get enough of this T-shirt! The fabric is so soft and comfortable that it feels like a second skin. The fit is amazing—it’s neither too tight nor too loose, just right. I've worn it many times, and it still looks like new. Definitely worth every penny!* Another review of a T-shirt:*\")\n\nfor i in range(0, len(prompts)):\n    print(f\"Prompt {i+1}: perplexity: {calculate_perplexity(prompts[i], pipeline.model, pipeline.tokenizer)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:03:26.345565Z","iopub.execute_input":"2024-08-19T22:03:26.346363Z","iopub.status.idle":"2024-08-19T22:03:27.549978Z","shell.execute_reply.started":"2024-08-19T22:03:26.346329Z","shell.execute_reply":"2024-08-19T22:03:27.549010Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Negative examples in prompt","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\nprompts = []\nprompts.append(\"Here is a review of a T-shirt: *I was really disappointed with this T-shirt. The fabric feels cheap and rough on my skin. After just one wash, it started to shrink and lose its shape. The color faded quickly, and now it looks old and worn out. I expected better quality for the price.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *Unfortunately, this T-shirt did not live up to my expectations. The material feels scratchy and uncomfortable, and the fit is awkward. It's either too tight in some areas or too loose in others. I've only worn it a few times, but it's already showing signs of wear. Not happy with this purchase at all.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *I regret buying this T-shirt. The fabric is stiff and uncomfortable, and it doesn't breathe well, making it unsuitable for warmer weather. The stitching started coming apart after a couple of washes, and it just looks cheap overall. I would not recommend this shirt to anyone.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt was a total letdown. The fabric feels thin and flimsy, and the fit is all wrong. After just a few wears, it stretched out and became shapeless. The quality is definitely lacking, and it didn't hold up in the wash at all. I won’t be buying from this brand again.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *I had high hopes for this T-shirt, but it fell short in every way. The material feels low-quality and rough, and the seams are already coming undone. It also shrank significantly after the first wash, making it unwearable. Save your money and avoid this one.* Another review of a T-shirt: *\")\n\nfor i in range(0, len(prompts)):\n    print(f\"Prompt {i+1}: perplexity: {calculate_perplexity(prompts[i], pipeline.model, pipeline.tokenizer)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:03:36.921587Z","iopub.execute_input":"2024-08-19T22:03:36.922311Z","iopub.status.idle":"2024-08-19T22:03:38.124222Z","shell.execute_reply.started":"2024-08-19T22:03:36.922271Z","shell.execute_reply":"2024-08-19T22:03:38.123188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Neutral examples in prompt","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()\nprompts = []\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt is okay, nothing special. The fabric is decent, but it's not as soft as I was hoping. It fits alright, though it could be a bit more flattering. It's just an average T-shirt that gets the job done, but I probably won't be buying it again. It’s fine for everyday wear, but nothing to write home about.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt is alright, but it didn't quite meet my expectations. The material is fine, though it feels a bit on the thinner side. The fit is okay, but I had to size up to get the right fit. It's a decent shirt for the price, but I wouldn't go out of my way to buy it again.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *The T-shirt is pretty average. The material is comfortable enough, but it's not the softest I've owned. It fits reasonably well, but it doesn't stand out in any way. It's a basic shirt that works for casual wear, but it's not something I would rave about. Overall, it's fine for what it is.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt is decent. The fabric feels okay, and it fits well enough, but it's not particularly impressive. After a few washes, it still looks good, but there’s nothing that makes it stand out from other shirts I own. It’s a solid basic T-shirt, but I wouldn't call it a must-have.* Another review of a T-shirt: *\")\nprompts.append(\"Here is a review of a T-shirt: *This T-shirt is neither great nor terrible. The fabric is average, and the fit is just fine. It’s a basic shirt that serves its purpose, but there’s nothing special about it. It’s comfortable enough to wear casually, but I wouldn’t call it my favorite. It’s simply an okay shirt.* Another review of a T-shirt: *\")\n\nfor i in range(0, len(prompts)):\n    print(f\"Prompt {i+1}: perplexity: {calculate_perplexity(prompts[i], pipeline.model, pipeline.tokenizer)}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:03:49.589169Z","iopub.execute_input":"2024-08-19T22:03:49.589879Z","iopub.status.idle":"2024-08-19T22:03:50.796251Z","shell.execute_reply.started":"2024-08-19T22:03:49.589846Z","shell.execute_reply":"2024-08-19T22:03:50.795199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate 3350 reviews using positive prompt\npos_prompt = \"Here is a review of a T-shirt: *I absolutely love this T-shirt! The material is so soft, and it feels amazing against the skin. I've washed it multiple times, and it still looks brand new. The fit is perfect, and I get compliments every time I wear it. This has become my favorite go-to shirt for both casual and semi-casual outings. Highly recommended!* Another review of a T-shirt:*\"\nreview_set1 = []\n\nfor i in range(0, batches):\n    torch.cuda.empty_cache()\n    text = pipeline(\n        pos_prompt,\n        max_new_tokens = 200,\n        min_new_tokens = 50,\n        num_return_sequences=batch_size,\n        top_p=0.95,       # Nucleus sampling\n        pad_token_id = tokenizer.eos_token_id,\n        eos_token_id = tokenizer.eos_token_id\n    )\n\n    for output in text:\n        generated_text = output['generated_text']\n        result_text = generated_text[len(pos_prompt):].strip()\n        if \"*\" in result_text:\n            result_text = result_text.split(\"*\")[0].strip()\n        review_set1.append(result_text)\n        print(result_text)\n\ndf1 = pd.DataFrame(review_set1, columns=['Review'])\ndf1.to_csv(\"/kaggle/working/pos_sample.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:17:31.077930Z","iopub.execute_input":"2024-08-19T22:17:31.078745Z","iopub.status.idle":"2024-08-19T22:20:40.025135Z","shell.execute_reply.started":"2024-08-19T22:17:31.078711Z","shell.execute_reply":"2024-08-19T22:20:40.024009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate 3350 reviews using positive prompt\nneg_prompt = \"Here is a review of a T-shirt: *Unfortunately, this T-shirt did not live up to my expectations. The material feels scratchy and uncomfortable, and the fit is awkward. It's either too tight in some areas or too loose in others. I've only worn it a few times, but it's already showing signs of wear. Not happy with this purchase at all.* Another review of a T-shirt: *\"\nreview_set2 = []\n\nfor i in range(0, batches):\n    torch.cuda.empty_cache()\n    text = pipeline(\n        neg_prompt,\n        max_new_tokens = 200,\n        min_new_tokens = 50,\n        num_return_sequences=batch_size,\n        top_p=0.95,       # Nucleus sampling\n        pad_token_id = tokenizer.eos_token_id,\n        eos_token_id = tokenizer.eos_token_id\n    )\n\n    for output in text:\n        generated_text = output['generated_text']\n        result_text = generated_text[len(neg_prompt):].strip()\n        if \"*\" in result_text:\n            result_text = result_text.split(\"*\")[0].strip()\n        review_set2.append(result_text)\n        print(result_text)\n        \ndf2 = pd.DataFrame(review_set2, columns=['Review'])\ndf2.to_csv(\"/kaggle/working/neg_sample.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:24:03.732783Z","iopub.execute_input":"2024-08-19T22:24:03.733543Z","iopub.status.idle":"2024-08-19T22:27:12.561688Z","shell.execute_reply.started":"2024-08-19T22:24:03.733509Z","shell.execute_reply":"2024-08-19T22:27:12.560705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate 3350 reviews using neutral prompt\nntrl_prompt = \"Here is a review of a T-shirt: *This T-shirt is alright, but it didn't quite meet my expectations. The material is fine, though it feels a bit on the thinner side. The fit is okay, but I had to size up to get the right fit. It's a decent shirt for the price, but I wouldn't go out of my way to buy it again.* Another review of a T-shirt: *\"\nreview_set3 = []\n\nfor i in range(0, batches):\n    torch.cuda.empty_cache()\n    text = pipeline(\n        ntrl_prompt,\n        max_new_tokens = 200,\n        min_new_tokens = 50,\n        num_return_sequences=batch_size,\n        top_p=0.95,       # Nucleus sampling\n        pad_token_id = tokenizer.eos_token_id,\n        eos_token_id = tokenizer.eos_token_id\n    )\n\n    for output in text:\n        generated_text = output['generated_text']\n        result_text = generated_text[len(ntrl_prompt):].strip()\n        if \"*\" in result_text:\n            result_text = result_text.split(\"*\")[0].strip()\n        review_set3.append(result_text)\n        print(result_text)\n        \ndf3 = pd.DataFrame(review_set3, columns=['Review'])\ndf3.to_csv(\"/kaggle/working/ntrl_sample.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T22:29:57.198670Z","iopub.execute_input":"2024-08-19T22:29:57.199553Z","iopub.status.idle":"2024-08-19T22:33:06.141454Z","shell.execute_reply.started":"2024-08-19T22:29:57.199500Z","shell.execute_reply":"2024-08-19T22:33:06.140403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ntorch.cuda.empty_cache()\n\ndef generate(prompt, num_return_sequences=2, top_p=0.95, max_new_tokens=150, min_new_tokens=50):\n    text = pipeline(\n        prompt,\n        num_return_sequences=num_return_sequences,\n        top_p=top_p,  # Nucleus sampling\n        max_new_tokens=max_new_tokens,\n        min_new_tokens=min_new_tokens\n    )\n    # Extract and format the generated text\n    generated = [text[i]['generated_text'].strip() for i in range(len(text))]\n    \n    outputs = []\n    \n    for output in generated:\n        generated_text = output['generated_text'] if isinstance(output, dict) else output\n        start_idx = len(prompt)\n        result_text = generated_text[len(prompt):].strip()\n        print(result_text)\n        if \"#\" in result_text:\n            result_text = result_text.split(\"#\")[0].strip()\n        if \"+\" in result_text:\n            result_text = result_text.split(\"+\")[0].strip()\n        if \"Review 3\" in result_text:\n            result_text = result_text.split(\"Review 3\")[0].strip()\n        outputs.append(result_text)\n    #print(outputs)\n    return outputs\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-19T11:23:49.562251Z","iopub.execute_input":"2024-08-19T11:23:49.562627Z","iopub.status.idle":"2024-08-19T11:23:49.578267Z","shell.execute_reply.started":"2024-08-19T11:23:49.562597Z","shell.execute_reply":"2024-08-19T11:23:49.577365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport time\nstart = time.time()\nend = time.time()\nprint(end-start)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-19T11:23:56.496686Z","iopub.execute_input":"2024-08-19T11:23:56.497467Z","iopub.status.idle":"2024-08-19T11:23:56.502943Z","shell.execute_reply.started":"2024-08-19T11:23:56.497432Z","shell.execute_reply":"2024-08-19T11:23:56.501938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n#import torch\n\nmodel = pipeline.model\ntokenizer = pipeline.tokenizer\n\ndef calculate_perplexity(text, model, tokenizer):\n    # Tokenize the input text\n    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n\n    # Generate labels (using input_ids as the labels for language modeling)\n    input_ids = inputs['input_ids']\n    \n    # Compute the loss\n    with torch.no_grad():  # No need to track gradients\n        outputs = model(input_ids, labels=input_ids)\n    \n    # Extract the loss\n    loss = outputs.loss\n    print(loss)\n\n    # Convert the loss to perplexity\n    perplexity = torch.exp(loss).item()\n    return perplexity\n\n# Generate text\n#result = generate(\"This is a product review for a t-shirt: \", \n#                  num_return_sequences=5, \n#                  temperature=1.0, \n#                  top_p=0.9, \n#                  top_k=50)\n\n# Calculate perplexity for each generated text\nfor i, gen_text in enumerate(result):\n    perplexity = calculate_perplexity(gen_text, model, tokenizer)\n    print(f\"Generated Text {i+1}: {gen_text}\")\n    print(f\"Perplexity: {perplexity}\\n\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-08-19T10:05:06.865281Z","iopub.execute_input":"2024-08-19T10:05:06.865958Z","iopub.status.idle":"2024-08-19T10:05:53.122561Z","shell.execute_reply.started":"2024-08-19T10:05:06.865929Z","shell.execute_reply":"2024-08-19T10:05:53.121202Z"},"trusted":true},"execution_count":null,"outputs":[]}]}