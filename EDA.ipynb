{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2024-07-08T08:12:37.225804Z","iopub.status.busy":"2024-07-08T08:12:37.225341Z","iopub.status.idle":"2024-07-08T08:14:32.396201Z","shell.execute_reply":"2024-07-08T08:14:32.394698Z","shell.execute_reply.started":"2024-07-08T08:12:37.225759Z"},"trusted":true},"source":["# **Preliminary Data Analysis**\n","\n","This script performs exploratory data analysis (EDA) on our sample of Amazon fashion item reviews (available [here]()):\n","* Rating distribution (overall and by sentiment)\n","* Review length\n","* Word clouds\n","* Topic modeling using BERTopic\n","\n","## Preparations"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/tom/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n","  _torch_pytree._register_pytree_node(\n","[nltk_data] Downloading package punkt to /home/tom/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/tom/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Import packages\n","import re\n","from zipfile import ZipFile\n","import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import seaborn as sns\n","from datasets import load_dataset\n","from langdetect import detect, DetectorFactory\n","from transformers import set_seed\n","from collections import Counter\n","import nltk\n","from wordcloud import WordCloud\n","import string\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Download additional files for NLTK\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Set seeds for reproducible and consistent results\n","set_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load sampled training data\n","with ZipFile('Data/checkpoint3.zip', 'r') as zip:\n","    with zip.open('checkpoint3.csv') as file:\n","        reviews_sample = pd.read_csv(file)\n","        file.close()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load wholde dataset\n","with ZipFile('Data/checkpoint2.zip', 'r') as zip:\n","    with zip.open('checkpoint2.csv') as file:\n","        reviews_all = pd.read_csv(file)\n","        file.close()"]},{"cell_type":"markdown","metadata":{},"source":["# Whole Dataset"]},{"cell_type":"markdown","metadata":{},"source":["### Rating distribuition"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ratings = pd.DataFrame(reviews_all['rating'].value_counts())\n","\n","plt.figure(figsize=(10, 6))\n","# Plot bar graph using seaborn\n","sns.barplot(x='rating', y='count', data=ratings)\n","# Set plot title and labels\n","plt.title('Rating Distribution', weight='bold')\n","plt.xlabel('Rating', weight='bold')\n","plt.ylabel('Number of Reviews', weight='bold')\n","\n","#Save the plot\n","plt.savefig('Plots/rating_distribution_all.pdf',format='pdf', dpi=1500)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Sentiment distribution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Replace 1 with 'POS' and 0 with 'NEG'\n","reviews_all['label'].replace({1: 'POS', 0: 'NEG'}, inplace=True)\n","category_number = reviews_all['label'].value_counts()\n","print(category_number)\n","\n","category_percentages = reviews_all['label'].value_counts(normalize=True) * 100\n","print(category_percentages)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Group by Review_Score and Sentiment\n","sentiment_distribution = reviews_all.groupby(['rating','sentiment']).size().reset_index(name='Count')\n","sentiment_distribution "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sentiment distributions\n","plt.figure(figsize=(10, 6))\n","# Plot bar graph using seaborn\n","sns.barplot(x='rating', y='Count', hue='sentiment', data=sentiment_distribution, palette=['red', 'green'])\n","# Set plot title and labels\n","plt.title('Sentiment Distribution by Rating', weight='bold')\n","plt.xlabel('Rating', weight='bold')\n","plt.ylabel('Number of Reviews', weight='bold')\n","plt.legend(title='Sentiment')\n","\n","# Save the plot\n","plt.savefig('Plots/sentiment_dist_by_rating.pdf',format='pdf', dpi=1500)\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Traning data"]},{"cell_type":"markdown","metadata":{},"source":["## Review length\n","\n","### Distribution of Review Lengths"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Tokenize the reviews and determine their lengths\n","revs = reviews_sample['text'].copy()\n","\n","doc_lengths = []\n","\n","for rev in revs:\n","    tokens = nltk.word_tokenize(rev)\n","    doc_lengths.append(len(tokens))\n","\n","doc_lengths = np.array(doc_lengths)\n","\n","# Plot the distribution of review lengths (Figure A1 in the Appendix)\n","plt.figure(figsize=(10,8))\n","ax = sns.histplot(doc_lengths, binwidth=7, kde=True)\n","ax.lines[0].set_color('black')\n","plt.ylabel('Number of Reviews', weight='bold', fontsize=17)\n","plt.xticks(fontsize=14)\n","plt.yticks(fontsize=14)\n","plt.savefig('hist_rev_len.pdf', format='pdf', dpi=1500)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Review Lengths (Total and by Sentiment)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculations for text in Data chapter \n","print('Average length of all reviews:' + str(np.average(doc_lengths)))\n","print('Standard deviation of the length of all reviews:' + str(np.std(doc_lengths)))\n","\n","# Calculations for footnote in Data chapter\n","## Length of positive reviews\n","revs_pos = reviews_sample[reviews_sample['label'] == 1]['text'].copy()\n","\n","doc_lengths_pos = []\n","\n","for rev in revs_pos:\n","    tokens = nltk.word_tokenize(rev)\n","    doc_lengths_pos.append(len(tokens))\n","\n","doc_lengths_pos = np.array(doc_lengths_pos)\n","\n","## Length of negative reviews\n","revs_neg = reviews_sample[reviews_sample['label'] == 0]['text'].copy()\n","\n","doc_lengths_neg = []\n","\n","for rev in revs_neg:\n","    tokens = nltk.word_tokenize(rev)\n","    doc_lengths_neg.append(len(tokens))\n","\n","doc_lengths_neg = np.array(doc_lengths_neg)\n","\n","## Check average length and SD of length\n","print('\\n')\n","print('Average length of positive reviews:' + str(np.average(doc_lengths_pos)))\n","print('Standard deviation of the length of positive reviews:' + str(np.std(doc_lengths_pos)))\n","\n","print('\\n')\n","print('Average length of negative reviews:' + str(np.average(doc_lengths_neg)))\n","print('Standard deviation of the length of negative reviews:' + str(np.std(doc_lengths_neg)))"]},{"cell_type":"markdown","metadata":{},"source":["## Rating Distribution\n","\n","### Overall Rating Distribution"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract number of reviews by ratings\n","ratings = pd.DataFrame(reviews_sample['rating'].value_counts())\n","\n","# Plot number of reviews by ratings\n","plt.figure(figsize=(10, 6))\n","sns_barplot = sns.barplot(x='rating', y='count', data=ratings)\n","for bar in sns_barplot.patches:\n","    bar.set_edgecolor('black')  # Set the border color\n","    bar.set_linewidth(1)        # Set the border thickness\n","#plt.title('Rating Distribution', weight='bold')\n","plt.xlabel('Rating', weight='bold', fontsize=17)\n","plt.ylabel('Number of Reviews', weight='bold', fontsize=17)\n","plt.xticks(fontsize=14)\n","plt.yticks(fontsize=14)\n","plt.ylim(top=34000)\n","plt.savefig('rating_distribution.pdf', format='pdf', dpi=1500)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Rating Distribution by Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extract number of reviews by ratings and sentiment\n","value_counts = reviews_sample.groupby(['rating', 'label']).size().reset_index(name='counts')\n","\n","# Plot number of reviews by ratings and senitment\n","plt.figure(figsize=(10, 6))\n","sns_barplot = sns.barplot(data=value_counts, x='rating', y='counts', hue='label', palette={0: 'red', 1: 'green'})\n","for bar in sns_barplot.patches:\n","    bar.set_edgecolor('black')  # Set the border color\n","    bar.set_linewidth(1)        # Set the border thickness\n","#plt.title('Rating Distribution', weight='bold')\n","plt.xlabel('Rating', weight='bold', fontsize=17)\n","plt.ylabel('Number of Reviews', weight='bold', fontsize=17)\n","plt.xticks(fontsize=14)\n","plt.yticks(fontsize=14)\n","plt.ylim(top=34000)\n","handles, labels = plt.gca().get_legend_handles_labels()\n","custom_labels = ['Negative', 'Positive']\n","plt.legend(title='Sentiment', handles=handles, labels=custom_labels, title_fontsize=16, fontsize=14)\n","plt.savefig('rating_distribution_sentiment.pdf',format='pdf', dpi=1500)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Word Clouds\n","\n","### TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["revs = reviews_sample['text'].copy()\n","\n","# Create the vectorizer\n","vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n","\n","# Fit and transform the documents\n","tfidf_matrix = vectorizer.fit_transform(revs)\n","\n","# Get the feature names (the terms)\n","feature_names = vectorizer.get_feature_names_out()\n","\n","# Convert the matrix to an array for easier viewing\n","tfidf_array = tfidf_matrix.toarray()\n","\n","# Print the TF-IDF matrix\n","print(\"TF-IDF Matrix:\")\n","print(tfidf_array)\n","\n","# Print the feature names\n","print(\"Feature Names:\")\n","print(feature_names)"]},{"cell_type":"markdown","metadata":{},"source":["### Word Cloud for all Reviews"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sum the TF-IDF scores for each term across all documents\n","tfidf_scores = tfidf_matrix.sum(axis=0).A1  # .A1 flattens the matrix into an array\n","\n","# Create a dictionary of words and their corresponding TF-IDF scores\n","tfidf_dict = dict(zip(feature_names, tfidf_scores))\n","\n","# Generate the word cloud\n","wordcloud = WordCloud(width=4000, height=2000, background_color='white', colormap='viridis').generate_from_frequencies(tfidf_dict)\n","\n","# Display the word cloud\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","plt.tight_layout()\n","plt.savefig('wordcloud.pdf', format='pdf', dpi=1000)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Word Clouds by Sentiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to generate a word cloud for a specific sentiment\n","def generate_wordcloud_for_sentiment(df, column, value):\n","    # Filter the DataFrame for the selected sentiment\n","    sentiment_data = df[df[column] == value]\n","    \n","    # Combine all documents in the filtered DataFrame into one text block\n","    documents = sentiment_data['text']\n","    \n","    # Create TF-IDF vectorizer\n","    vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n","    \n","    # Fit and transform the documents\n","    tfidf_matrix = vectorizer.fit_transform(documents)\n","    \n","    # Get the feature names (terms)\n","    feature_names = vectorizer.get_feature_names_out()\n","    \n","    # Sum the TF-IDF scores for each term\n","    tfidf_scores = tfidf_matrix.sum(axis=0).A1  # .A1 flattens the matrix\n","    \n","    # Create a dictionary mapping words to their corresponding TF-IDF scores\n","    tfidf_dict = dict(zip(feature_names, tfidf_scores))\n","    \n","    # Generate the word cloud\n","    wordcloud = WordCloud(width=4000, height=2000, background_color='black').generate_from_frequencies(tfidf_dict)\n","    \n","    # Display and save the word cloud\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    if column == 'label':\n","        if value == 1:\n","            plt.title('Positive', weight='bold', fontsize=20)\n","        else:\n","            plt.title('Negative', weight='bold', fontsize=20)\n","    else:\n","        plt.title(f'Rating: {value}', weight='bold', fontsize=20)\n","    plt.axis('off')\n","    plt.tight_layout()\n","    if column == 'label':\n","        if value == 1:\n","            plt.savefig('wordcloud_pos.pdf', format='pdf', dpi=1000)\n","        else:\n","            plt.savefig('wordcloud_neg.pdf', format='pdf', dpi=1000)\n","    else:\n","        plt.savefig(f'wordcloud_rating{value}.pdf', format='pdf', dpi=1000)\n","    plt.show()\n","\n","# Generate word clouds for positive and negative sentiments\n","generate_wordcloud_for_sentiment(reviews_sample, 'label', 1) # positive\n","generate_wordcloud_for_sentiment(reviews_sample, 'label', 0) # negative"]},{"cell_type":"markdown","metadata":{},"source":["### Word Clouds by Rating"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate word clouds for all ratings\n","generate_wordcloud_for_sentiment(reviews_sample, 'rating', 1)\n","generate_wordcloud_for_sentiment(reviews_sample, 'rating', 2)\n","generate_wordcloud_for_sentiment(reviews_sample, 'rating', 3)\n","generate_wordcloud_for_sentiment(reviews_sample, 'rating', 4)\n","generate_wordcloud_for_sentiment(reviews_sample, 'rating', 5)"]},{"cell_type":"markdown","metadata":{},"source":["## Topic Modelling with BERTopic\n","\n","As the topic modelling requires GPUs to run the procedure is run on kaggle with the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import warnings\n","\n","import spacy\n","import nltk\n","import pandas as pd\n","from bertopic import BERTopic\n","from bertopic.representation import KeyBERTInspired\n","from bertopic.representation import MaximalMarginalRelevance\n","from bertopic.representation import TextGeneration\n","from bertopic.vectorizers import ClassTfidfTransformer\n","# Changed to CPU supported modules due to unavailability \n","#from cuml.cluster import HDBSCAN\n","#from cuml.manifold import UMAP\n","from hdbscan import HDBSCAN\n","import umap\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","from transformers import AutoTokenizer\n","from transformers import pipeline, set_seed\n","\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)      \n","set_seed(42)\n","\n","class BertopicModel:\n","\n","    def __init__(self, nr_topics):\n","\n","        self.nr_topics = nr_topics\n","        # Prepare stopwords list\n","        nltk.download('stopwords')\n","        self.stop_words = set(stopwords.words('english'))\n","        self.vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=list(\n","            self.stop_words))  # max_df=0.90, min_df=0.005) #percentage threshold to remove words based on occurence in documents\n","        self.tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n","\n","    def clean(self, df, column):\n","        df = df.loc[df[column].notnull(), :]\n","        documents = df[column].to_list()\n","        return df, documents\n","\n","    def get_representation_model(self):\n","\n","        prompt = \"\"\"I have a topic described by the following keywords: [KEYWORDS] and  [Documents]\n","\n","                    Based on the previous keywords, what is this topic about?\"\"\"\n","\n","        # Create your representation model\n","        generator = pipeline('text2text-generation',\n","                                model='google/flan-t5-large')\n","        representation_model_text_generation = TextGeneration(\n","            generator, prompt=prompt)\n","        representation_model_keybert = KeyBERTInspired()\n","        representation_model_mmm = MaximalMarginalRelevance(diversity=0.25)\n","        representation_model = [representation_model_mmm, representation_model_keybert,\n","                                representation_model_text_generation]\n","        return representation_model\n","\n","    def topic_model(self):\n","        ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n","        # embedding = tokenizer(documents.to_list(), padding=True, truncation=True, max_length=1024, return_tensors='pt')\n","        # Create instances of GPU-accelerated UMAP and HDBSCAN\n","        umap_model = umap.UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n","        # Optimize UMAP parameters for memory usage\n","        #umap_model = umap.UMAP(n_components=2, n_neighbors=10, min_dist=0.0, low_memory=True)\n","        hdbscan_model = HDBSCAN(\n","            min_samples=10, gen_min_span_tree=True, prediction_data=True)\n","\n","        topic_model = BERTopic(\"english\",\n","                                embedding_model=self.tokenizer,\n","                                verbose=True,\n","                                nr_topics=self.nr_topics,  # check\n","                                top_n_words=25,\n","                                representation_model=self.get_representation_model(),\n","                                vectorizer_model=self.vectorizer_model,\n","                                ctfidf_model=ctfidf_model,\n","                                umap_model=umap_model,\n","                                hdbscan_model=hdbscan_model\n","                                )\n","        return topic_model\n","\n","\n","    def run(self, input_path, output_path1, output_path2, column, reduce_outliers=True,\n","        strategy=\"embeddings\"):  # reduce_outliers #optional\n","        df = pd.read_csv(input_path)\n","        df, documents = self.clean(df=df, column=column)\n","        model = self.topic_model()\n","        # documents_list = documents.to_list()\n","        topics, probs = model.fit_transform(documents=documents)\n","        if reduce_outliers:\n","            # Reduce outliers using the embeddings strategy\n","            print(\"Running outlier reduction\")\n","            reduced_topics = model.reduce_outliers(\n","                documents, topics, strategy=strategy)\n","            model.update_topics(\n","                documents, topics=reduced_topics, vectorizer_model=self.vectorizer_model)\n","        topic_info = model.get_topic_info()\n","        topic_info.to_csv(output_path1, index=False)  \n","        document_info = model.get_document_info(documents)\n","        document_info.to_csv(output_path2, index=False)\n","\n","\n","# define hyperparameters\n","nr_topics = 15\n","input_data = '/kaggle/input/checkpoint3/checkpoint3.csv'\n","output_path1 = \"text_topics.csv\"   \n","output_path2 = \"text_clustered.csv\" \n","column = \"text\"\n","        \n","\n","# run topic modelling\n","topic_model = BertopicModel(nr_topics=nr_topics)  \n","topic_model.run(input_path=input_data, output_path1=output_path1, output_path2=output_path2, column=column)"]},{"cell_type":"markdown","metadata":{},"source":["### TM Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with ZipFile('Data/topic_models.zip', 'r') as zip:\n","        # Topic modelling with review content\n","        with zip.open('text_topics.csv') as file:\n","                text_topics = pd.read_csv(file) \n","                file.close()\n","                \n","display(text_topics)        "]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5350878,"sourceId":8900666,"sourceType":"datasetVersion"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
