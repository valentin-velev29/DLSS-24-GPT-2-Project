{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "from bertopic.representation import TextGeneration\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "# Changed to CPU supported modules due to unavailability of CUDA pip wheel in GCR environment\n",
    "# from cuml.cluster import HDBSCAN\n",
    "# from cuml.manifold import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "import umap\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "class BertopicModel:\n",
    "\n",
    "    def _init_(self, nr_topics):\n",
    "\n",
    "        self.nr_topics = nr_topics\n",
    "        # Prepare stopwords list\n",
    "        nltk.download('stopwords')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.vectorizer_model = CountVectorizer(ngram_range=(1, 1), stop_words=list(\n",
    "            self.stop_words))  # max_df=0.90, min_df=0.005) #percentage threshold to remove words based on occurence in documents\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "\n",
    "    def clean(self, df, column):\n",
    "        df = df.loc[df[column].notnull(), :]\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        # lemmatize:\n",
    "        documents = []\n",
    "        for feedback in df[column]:\n",
    "            document = nlp(feedback)\n",
    "            lemmatized_feedback = \" \".join([token.lemma_ for token in document])\n",
    "            documents.append(lemmatized_feedback)\n",
    "        return df, documents\n",
    "\n",
    "    def get_representation_model(self):\n",
    "\n",
    "        prompt = \"\"\"I have a topic described by the following keywords: [KEYWORDS] and  [Documents]\n",
    "\n",
    "                    Based on the previous keywords, what is this topic about?\"\"\"\n",
    "\n",
    "        # Create your representation model\n",
    "        generator = pipeline('text2text-generation',\n",
    "                                model='google/flan-t5-large')\n",
    "        representation_model_text_generation = TextGeneration(\n",
    "            generator, prompt=prompt)\n",
    "        representation_model_keybert = KeyBERTInspired()\n",
    "        representation_model_mmm = MaximalMarginalRelevance(diversity=0.25)\n",
    "        representation_model = [representation_model_mmm, representation_model_keybert,\n",
    "                                representation_model_text_generation]\n",
    "        return representation_model\n",
    "\n",
    "    def topic_model(self):\n",
    "        ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "        # embedding = tokenizer(documents.to_list(), padding=True, truncation=True, max_length=1024, return_tensors='pt')\n",
    "        # Create instances of GPU-accelerated UMAP and HDBSCAN\n",
    "        umap_model = umap.UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "        hdbscan_model = HDBSCAN(\n",
    "            min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "\n",
    "        topic_model = BERTopic(\"english\",\n",
    "                                embedding_model=self.tokenizer,\n",
    "                                verbose=True,\n",
    "                                nr_topics=self.nr_topics,  # check\n",
    "                                top_n_words=25,\n",
    "                                representation_model=self.get_representation_model(),\n",
    "                                vectorizer_model=self.vectorizer_model,\n",
    "                                ctfidf_model=ctfidf_model,\n",
    "                                umap_model=umap_model,\n",
    "                                hdbscan_model=hdbscan_model\n",
    "                                )\n",
    "        return topic_model\n",
    "\n",
    "    def run(self, input_path, output_path1, output_path2, column, reduce_outliers=True,\n",
    "            strategy=\"embeddings\"):  # reduce_outliers #optional\n",
    "        df = pd.read_csv(input_path)\n",
    "        df, documents = self.clean(df=df, column=column)\n",
    "        model = self.topic_model()\n",
    "        # documents_list = documents.to_list()\n",
    "        topics, probs = model.fit_transform(documents=documents)\n",
    "        if reduce_outliers:\n",
    "            # Reduce outliers using the embeddings strategy\n",
    "            print(\"Running outlier reduction\")\n",
    "            reduced_topics = model.reduce_outliers(\n",
    "                documents, topics, strategy=strategy)\n",
    "            model.update_topics(\n",
    "                documents, topics=reduced_topics, vectorizer_model=self.vectorizer_model)\n",
    "        topic_info = model.get_topic_info()\n",
    "        topic_info.to_csv(output_path1, index=False)  \n",
    "        document_info = model.get_documents()\n",
    "        document_info.to_csv(output_path2, index=False)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "nr_topics = 10\n",
    "input_data =\n",
    "output_path1 = \"topics.csv\"   \n",
    "output_path2 = \"documents_custered.csv\" \n",
    "column = \"\"\n",
    "        \n",
    "\n",
    "# run topic modelling\n",
    "topic_model = BertopicModel(nr_topics=nr_topics)  # get input from get_topic function\n",
    "topic_model.run(input_path=input_data, output_path1=, column=column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
